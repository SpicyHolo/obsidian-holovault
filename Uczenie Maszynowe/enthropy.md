Entropia, to funkcja przyrostu informacji.
$$\sum^c_{i=1}=-p_i \cdot log_2(p_i)$$
Gdzie $c$ -  liczba klas, $p_i$ - prawdopodobieństwo klasy $i$

### entropia krzyżowa
Wykorzystywana jako [[cost function]] dla klasyfikatorach w sieciach neuronowych.
$$J = -\displaystyle\frac{1}{N}\left( \sum^N_{i=1}\boldsymbol{y_i} \cdot log(\boldsymbol{\hat{y_i}}) \right )$$
#optimalisation 